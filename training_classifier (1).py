# -*- coding: utf-8 -*-
"""Training classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12lWJTW4rAfWKij2MpQp5H-to_aJZJ9nj
"""

!pip install -q tensorflow==2.12.0 keras==2.12.0
!pip install -q transformers==4.31.0 datasets==2.12.0

import pandas as pd
from datasets import Dataset


df = pd.read_csv("/content/financial_classifier_dataset.csv")

print(df.head())

dataset = Dataset.from_pandas(df)

dataset = dataset.train_test_split(test_size=0.2)
train_dataset = dataset["train"]
val_dataset = dataset["test"]

from transformers import DistilBertTokenizer

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_batch(example):
    return tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

tokenized_train = train_dataset.map(tokenize_batch, batched=True)
tokenized_val = val_dataset.map(tokenize_batch, batched=True)

import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

train_df = tokenized_train.to_pandas()
val_df = tokenized_val.to_pandas()

max_length = max(train_df['input_ids'].apply(len).max(), val_df['input_ids'].apply(len).max())

train_input_ids = pad_sequences(train_df['input_ids'], maxlen=max_length, padding='post')
train_attention_mask = pad_sequences(train_df['attention_mask'], maxlen=max_length, padding='post')
train_labels = np.array(train_df['label'])

val_input_ids = pad_sequences(val_df['input_ids'], maxlen=max_length, padding='post')
val_attention_mask = pad_sequences(val_df['attention_mask'], maxlen=max_length, padding='post')
val_labels = np.array(val_df['label'])

train_dataset = tf.data.Dataset.from_tensor_slices((
    {'input_ids': train_input_ids, 'attention_mask': train_attention_mask}, train_labels
)).shuffle(len(train_labels)).batch(16)

val_dataset = tf.data.Dataset.from_tensor_slices((
    {'input_ids': val_input_ids, 'attention_mask': val_attention_mask}, val_labels
)).batch(16)

from transformers import TFDistilBertForSequenceClassification
from tensorflow.keras.optimizers.legacy import Adam


model = TFDistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2
)


optimizer = Adam(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metrics = ['accuracy']

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=1
)

/

model.save_pretrained("./distilbert-financial-model")
tokenizer.save_pretrained("./distilbert-financial-model")

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=3
)

loss, accuracy = model.evaluate(val_dataset)
print(f"Validation Loss: {loss:.4f}")
print(f"Validation Accuracy: {accuracy:.4f}")

model.save_pretrained("./distilbert-financial-model")
tokenizer.save_pretrained("./distilbert-financial-model")

def predict(text):
    tokens = tokenizer(text, return_tensors="tf", padding=True, truncation=True)
    logits = model(tokens)[0]
    predicted_class = tf.argmax(logits, axis=1).numpy()[0]
    return "Financial" if predicted_class == 1 else "Non-Financial"

print(predict("plan my budget?"))
print(predict("fashion tips ?"))

model.save_pretrained("./distilbert-financial-model")
tokenizer.save_pretrained("./distilbert-financial-model")

import shutil
shutil.make_archive("distilbert-financial-model", 'zip', "./distilbert-financial-model")

from google.colab import files
files.download("distilbert-financial-model.zip")

